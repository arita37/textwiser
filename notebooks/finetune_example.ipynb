{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Finetune Example\n",
    "\n",
    "TextWiser is designed with extensibility and optimizability in mind. As such, it tries to allow fine-tuning for embeddings that are compatible. The detailed list is available in the README, and we will be using the FastText word embeddings for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the pipeline example, we use the news group dataset from Scikit-learn. This dataset contains 20 news groups with the aim of classifying a text document into one of these news groups. Here, we only use a subset of all the news group for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 2034\n",
      "Test data size: 1353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "print(\"Train data size: {}\".format(len(newsgroups_train.data)))\n",
    "print(\"Test data size: {}\".format(len(newsgroups_test.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward without fine-tuning\n",
    "\n",
    "We use a simple feedforward network which uses the word embeddings as a feature extractor, and builds a linear layer on top for classification. The only non-linearity is the `max` operation done on the word pooling stage. This architecture is analogous to using multi-class Logistic Regression in Scikit-learn. For this model, we do not allow fine-tuning of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): TextWiser(\n",
       "    (model): _Sequential(\n",
       "      (0): _WordEmbeddings(\n",
       "        (model): Embedding(1000001, 300, sparse=True)\n",
       "      )\n",
       "      (1): _PoolTransformation()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=300, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from textwiser import TextWiser, Embedding, PoolOptions, Transformation, WordOptions\n",
    "\n",
    "featurizer = TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained='en'), Transformation.Pool(pool_option=PoolOptions.max), dtype=torch.float32)\n",
    "featurizer.fit()  # Initialize the model\n",
    "clf = nn.Sequential(featurizer, nn.Linear(300, 4))\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the dataset more in-line with PyTorch best practices, we wrap it up in a PyTorch data loader, which takes care of batching and shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SklearnData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = data.data\n",
    "        self.y = torch.from_numpy(data.target)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.X[key], self.y[key]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=SklearnData(newsgroups_train),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=SklearnData(newsgroups_test),\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the testing procedure. It calculates both the cross entropy loss, and the F1-score (as used to evaluate the logistic regression module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.4290459070886885\n",
      "F1: 0.1308254526998125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doruk/miniconda3/envs/flair/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def run_test(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0\n",
    "        y_preds = []\n",
    "        for i, (X, y_act) in enumerate(test_loader):\n",
    "            y_pred = model(X)\n",
    "            y_preds.append(y_pred)\n",
    "            loss = criterion(y_pred, y_act)\n",
    "            running_loss += loss.item()\n",
    "        print(\"Test loss: {}\".format(running_loss / i))\n",
    "        print(\"F1: {}\".format(metrics.f1_score(newsgroups_test.target, torch.cat(y_preds).argmax(dim=1), average='macro')))\n",
    "\n",
    "run_test(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training procedure is defined in a similar way to the testing procedure, we just make sure that the gradients are properly backpropogated. We train the model for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3728034590917921\n",
      "Test loss: 1.3427254075095767\n",
      "F1: 0.15902572180794858\n",
      "Train loss: 1.3013285720159138\n",
      "Test loss: 1.312653694834028\n",
      "F1: 0.46820457930384674\n",
      "Train loss: 1.2439212912604922\n",
      "Test loss: 1.2670719538416182\n",
      "F1: 0.2387517378408087\n",
      "Train loss: 1.180820141519819\n",
      "Test loss: 1.183376139118558\n",
      "F1: 0.6304858479190042\n",
      "Train loss: 1.1273101492533608\n",
      "Test loss: 1.1516293116978236\n",
      "F1: 0.4237316569477765\n",
      "Train loss: 1.0902921765569658\n",
      "Test loss: 1.1317261826424372\n",
      "F1: 0.3940073084957095\n",
      "Train loss: 1.0442262490590413\n",
      "Test loss: 1.095052597068605\n",
      "F1: 0.48166709724363393\n",
      "Train loss: 1.0046795852600583\n",
      "Test loss: 1.0513577262560527\n",
      "F1: 0.7062689281993478\n",
      "Train loss: 0.9733775863571773\n",
      "Test loss: 1.0385798868678866\n",
      "F1: 0.5791997399619029\n",
      "Train loss: 0.9416219478561765\n",
      "Test loss: 1.0035120660350436\n",
      "F1: 0.5490690780697897\n",
      "Train loss: 0.9144637215705145\n",
      "Test loss: 0.9927101759683519\n",
      "F1: 0.5174257996209894\n",
      "Train loss: 0.8900331466917007\n",
      "Test loss: 0.9795907948698316\n",
      "F1: 0.5478367245980903\n",
      "Train loss: 0.8678427773808676\n",
      "Test loss: 0.9125716374033973\n",
      "F1: 0.7196161222766928\n",
      "Train loss: 0.8453401573120601\n",
      "Test loss: 0.8956250718661717\n",
      "F1: 0.7309355954934373\n",
      "Train loss: 0.8226510816150241\n",
      "Test loss: 0.878325811454228\n",
      "F1: 0.6877484112152741\n",
      "Train loss: 0.8054352421609182\n",
      "Test loss: 0.8730264377026331\n",
      "F1: 0.6268993906581145\n",
      "Train loss: 0.7884210669805133\n",
      "Test loss: 0.8536109654676347\n",
      "F1: 0.6792267516827375\n",
      "Train loss: 0.7755816664014544\n",
      "Test loss: 0.8358826168945858\n",
      "F1: 0.7117560440008082\n",
      "Train loss: 0.7551821517565894\n",
      "Test loss: 0.85936553137643\n",
      "F1: 0.6678103468852417\n",
      "Train loss: 0.744913711434319\n",
      "Test loss: 0.8211660413515001\n",
      "F1: 0.7062513236331338\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def run_train(model, epochs=20):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i, (X, y_act) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y_act)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Train loss: {}\".format(running_loss / i))\n",
    "        run_test(model)\n",
    "\n",
    "run_train(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward with fine-tuning\n",
    "\n",
    "Now that we have some results for a model without any fine-tuning on the word embeddings, we can try to get some idea on how fine-tuning the word vectors can have an impact on the results. For that, we set the `is_finetuneable` parameter to `True`, allowing the word embeddings to get updated. We also set the `sparse` parameter to `True` to make the computation faster, as we are using regular SGD and regular SGD supports sparse embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3728999618499997\n",
      "Test loss: 1.4624727283205305\n",
      "F1: 0.11276473955352032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doruk/miniconda3/envs/flair/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3004483419751365\n",
      "Test loss: 1.2668068494115556\n",
      "F1: 0.3533541672575323\n",
      "Train loss: 1.2066675784095886\n",
      "Test loss: 1.2630064572606767\n",
      "F1: 0.3464395516838561\n",
      "Train loss: 1.1065468021801539\n",
      "Test loss: 1.1482909563041868\n",
      "F1: 0.4207929749905942\n",
      "Train loss: 1.001804190022605\n",
      "Test loss: 1.0175516733101435\n",
      "F1: 0.625650046596664\n",
      "Train loss: 0.8983037112251161\n",
      "Test loss: 0.928099608137494\n",
      "F1: 0.6468610852745489\n",
      "Train loss: 0.7932572014748104\n",
      "Test loss: 0.8646905847958156\n",
      "F1: 0.6327623693340367\n",
      "Train loss: 0.7156903781588115\n",
      "Test loss: 0.7822205083710807\n",
      "F1: 0.7439485897126473\n",
      "Train loss: 0.6325573684677245\n",
      "Test loss: 0.7522109895944595\n",
      "F1: 0.7074469397543486\n",
      "Train loss: 0.5773770634144072\n",
      "Test loss: 0.7108256540128163\n",
      "F1: 0.684786043075462\n",
      "Train loss: 0.5200203449007065\n",
      "Test loss: 0.6587015042702357\n",
      "F1: 0.7621398303572542\n",
      "Train loss: 0.4740950380052839\n",
      "Test loss: 0.637218890445573\n",
      "F1: 0.7742645720492037\n",
      "Train loss: 0.43589949584196486\n",
      "Test loss: 0.6126030186812083\n",
      "F1: 0.7325757799254652\n",
      "Train loss: 0.3959967962333134\n",
      "Test loss: 0.605927308400472\n",
      "F1: 0.7829337607326122\n",
      "Train loss: 0.36473042695295244\n",
      "Test loss: 0.5593426397868565\n",
      "F1: 0.7912038926887799\n",
      "Train loss: 0.338005303153916\n",
      "Test loss: 0.5410707415569396\n",
      "F1: 0.7950656606872981\n",
      "Train loss: 0.3126660871600348\n",
      "Test loss: 0.5266955026558467\n",
      "F1: 0.7945101352935342\n",
      "Train loss: 0.29000902436082326\n",
      "Test loss: 0.5120664664677211\n",
      "F1: 0.8036911380694609\n",
      "Train loss: 0.26504237216616433\n",
      "Test loss: 0.5008790613639922\n",
      "F1: 0.8045007672887625\n",
      "Train loss: 0.24614035113463326\n",
      "Test loss: 0.48903273329848335\n",
      "F1: 0.8109741601362417\n"
     ]
    }
   ],
   "source": [
    "featurizer = TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained='en', sparse=True), Transformation.Pool(pool_option=PoolOptions.max),\n",
    "                       dtype=torch.float32, is_finetuneable=True)\n",
    "featurizer.fit()  # Initialize the model\n",
    "clf = nn.Sequential(featurizer, nn.Linear(300, 4))\n",
    "\n",
    "run_train(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the top F1 score with fine-tuning is better than the top F1 score without fine-tuning. Obviously, this makes the word embeddings lose some of their generality, which is necessary to get better scores in downstream tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
