{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Model Selection Example\n",
    "\n",
    "Textwiser is designed with rapid prototyping in mind, meaning it fully cooperates with the Scikit-learn model selection classes such as [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). For finding the best hyperparameters for a task, random search and grid search are two very popular techniques. This often involves going through different parameters of one specific model, such as the minimum document frequency in Tf-Idf. However, we can go one step further and treat all text featurization techniques as hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use the news group dataset from Scikit-learn. This dataset contains 20 news groups with the aim of classifying a text document into one of these news groups. Here, we only use a subset of all the news group for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 2034\n",
      "Test data size: 1353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "print(\"Train data size: {}\".format(len(newsgroups_train.data)))\n",
    "print(\"Test data size: {}\".format(len(newsgroups_test.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows how to set up regular hyperparameter search on the text featurization and on the classifier. The specified embedding and transformations are held internally inside the `embedding` and `transformations` variables respectively. Here we set the choices of the `min_df` parameter of the TfIdf embedding. For setting a parameter of a transformation, you have to index into the `transformations` variable. The object hierarchy is specified by the double underscore (`__`) separator, same as the Scikit-learn behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7174741300461003\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('featurizer',\n",
       "                 TextWiser(\n",
       "  (_imp): _Sequential(\n",
       "    (0): _TfIdfEmbeddings()\n",
       "    (1): _NMFTransformation()\n",
       "  )\n",
       ")),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=0.8888647751187911, class_weight=None,\n",
       "                                    dual=False, fit_intercept=True,\n",
       "                                    intercept_scaling=1, l1_ratio=None,\n",
       "                                    max_iter=400, multi_class='auto',\n",
       "                                    n_jobs=None, penalty='l2',\n",
       "                                    random_state=None, solver='lbfgs',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from textwiser import TextWiser, Embedding, PoolOptions, Transformation, WordOptions\n",
    "\n",
    "clf = Pipeline([('featurizer', TextWiser(Embedding.TfIdf(min_df=5), Transformation.NMF(n_components=30), lazy_load=True)),\n",
    "                ('classifier', LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=400))])\n",
    "\n",
    "param_dist = {'featurizer__embedding__min_df': [5, 10],\n",
    "              'featurizer__transformations__0__n_components': [20, 25, 30],\n",
    "              'classifier__C': uniform()}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring='f1_macro',\n",
    "                                   n_iter=n_iter_search, cv=5, iid=False)\n",
    "\n",
    "random_search.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "print(\"Best score: {}\".format(random_search.best_score_))\n",
    "print(\"Best model:\")\n",
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also treat the different feature extractors and classification models hyperparameters, and do a random search over them. Note that we also set the `lazy_load` parameter to `True`, meaning the model objects are only instantiated when either `fit` or `forward` is called. This saves a lot of memory at model selection stage, since initialized word embeddings and other models can use a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8392193843432569\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('featurizer',\n",
       "                 TextWiser(\n",
       "  (model): _Sequential(\n",
       "    (0): _WordEmbeddings(\n",
       "      (model): Embedding(1000001, 300, sparse=True)\n",
       "    )\n",
       "    (1): _PoolTransformation()\n",
       "  )\n",
       ")),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=400,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = Pipeline([('featurizer', None),\n",
    "                ('classifier', None)])\n",
    "\n",
    "param_dist = {'featurizer': [TextWiser(Embedding.TfIdf(min_df=5), Transformation.NMF(n_components=30), lazy_load=True), TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained='en'), Transformation.Pool(pool_option=PoolOptions.max), lazy_load=True)],\n",
    "              'classifier': [LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=400), DecisionTreeClassifier()]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring='f1_macro',\n",
    "                                   n_iter=n_iter_search, cv=5, iid=False)\n",
    "\n",
    "random_search.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "print(\"Best score: {}\".format(random_search.best_score_))\n",
    "print(\"Best model:\")\n",
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a more advanced example, we can define a schema as per the README, and modify the parameters inside the schema. This is important because we can theoretically swap entire embeddings and transformations using this method.\n",
    "\n",
    "We first create the schema to base our search on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"transform\": [\n",
    "        {\n",
    "            \"concat\": [\n",
    "                {\n",
    "                    \"transform\": [\n",
    "                        [\"word2vec\", {\"pretrained\": \"en\"}],\n",
    "                        \"pool\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"transform\": [\n",
    "                        \"tfidf\",\n",
    "                        [\"nmf\", { \"n_components\": 30 }]\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"svd\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pick any single component of the schema and give the usual hyperparameter options. You can go down the schema with the usual double underscore (`__`) separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8301270978191986\n",
      "Best model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('featurizer',\n",
       "                 TextWiser(\n",
       "  (model): _Sequential(\n",
       "    (0): _CompoundEmbeddings(\n",
       "      (model): _Sequential(\n",
       "        (0): _Concat(\n",
       "          (embeddings): ModuleList(\n",
       "            (0): _Sequential(\n",
       "              (0): _WordEmbeddings(\n",
       "                (model): Embedding(1000001, 300, sparse=True)\n",
       "              )\n",
       "              (1): _PoolTransformation()\n",
       "            )\n",
       "            (1): _Sequential(\n",
       "              (0): _TfIdfEmbeddings()\n",
       "              (1): _NMFTransformation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): _SVDTransformation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=0.7286697905299401, class_weight=None,\n",
       "                                    dual=False, fit_intercept=True,\n",
       "                                    intercept_scaling=1, l1_ratio=None,\n",
       "                                    max_iter=400, multi_class='auto',\n",
       "                                    n_jobs=None, penalty='l2',\n",
       "                                    random_state=None, solver='lbfgs',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([('featurizer', TextWiser(Embedding.Compound(schema=schema), dtype=np.float32, lazy_load=True)),\n",
    "                ('classifier', LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=400))])\n",
    "\n",
    "param_dist = {'featurizer__embedding__schema__transform__0__concat__1__transform__1__n_components': [5, 10],\n",
    "              'classifier__C': uniform()}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring='f1_macro',\n",
    "                                   n_iter=n_iter_search, cv=2, iid=False)\n",
    "\n",
    "random_search.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "print(\"Best score: {}\".format(random_search.best_score_))\n",
    "print(\"Best model:\")\n",
    "random_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
