{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Pipeline Example\n",
    "\n",
    "TextWiser is designed with rapid prototyping and scikit-learn interoperability in mind. As such, it supports integration with the [Scikit-learn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we use the news group dataset from Scikit-learn. This dataset contains 20 news groups with the aim of classifying a text document into one of these news groups. Here, we only use a subset of all the news group for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 2034\n",
      "Test data size: 1353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "print(\"Train data size: {}\".format(len(newsgroups_train.data)))\n",
    "print(\"Test data size: {}\".format(len(newsgroups_test.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Vectorization\n",
    "\n",
    "We use Tf-Idf as the embedding type, and do dimensionality reduction using Nonnegative Matrix Factorization. This represents a rather common use-case that is trivial to solve without relying on anything else than Scikit-learn.\n",
    "\n",
    "The only difference here is that we use the relevant TextWiser, Embedding, and Transformation objects to set up the text vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from textwiser import TextWiser, Embedding, Transformation\n",
    "\n",
    "emb = TextWiser(Embedding.TfIdf(min_df=5), Transformation.NMF(n_components=30))\n",
    "vecs = emb.fit_transform(newsgroups_train.data)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the documents are vectorized, we can train a multi-class Logistic Regression model to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(multi_class='auto', solver='lbfgs')\n",
    "clf.fit(vecs, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the test set, we can check the macro-averaged F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6787583044194434"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "vecs_test = emb.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vecs_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the pipeline\n",
    "\n",
    "Since this use-case can generally be defined as text featurization followed by classification, we can fit that idea into a `Pipeline` object. Here, we use FastText word embeddings by Facebook to first convert individual words into word vectors, and then do a pooling operation on them to get a single vector per document. This would normally require the user to download and set up the word embeddings from a different library, but here it is just another option to use in TextWiser.\n",
    "\n",
    "We again use a Logistic Regression classifier, train the whole pipeline with the training data, and get the F1 score of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7795074956936598"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from textwiser import TextWiser, Embedding, PoolOptions, Transformation, WordOptions\n",
    "\n",
    "clf = Pipeline([('featurizer', TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained='en'), Transformation.Pool(pool_option=PoolOptions.max))),\n",
    "                ('classifier', LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=400))])\n",
    "vecs = clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "pred = clf.predict(newsgroups_test.data)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
